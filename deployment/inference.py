import os
import json
import time
import torch
from chronos import ChronosBoltPipeline


# --------------------------
# CONFIGURACI√ìN GLOBAL
# --------------------------
MODEL_DIR = "/opt/ml/model"  # Aqu√≠ se monta tu modelo local


# --------------------------
# UTILIDADES
# --------------------------
def log(msg: str):
    """Helper para imprimir logs con timestamps (visible en CloudWatch o consola local)."""
    print(f"[Chronos] {time.strftime('%Y-%m-%d %H:%M:%S')} | {msg}", flush=True)


# --------------------------
# FUNCIONES DE INFERENCIA
# --------------------------
def model_fn(model_dir: str):
    """Carga el modelo Chronos desde el directorio local."""
    if not os.path.exists(model_dir):
        raise FileNotFoundError(f"‚ùå Modelo no encontrado en {model_dir}")

    log(f"üîπ Cargando modelo Chronos desde: {model_dir}")
    pipe = ChronosBoltPipeline.from_pretrained(model_dir, device_map="cpu")
    log("‚úÖ Modelo cargado con √©xito.")
    return pipe


def input_fn(request_body, content_type):
    """Parsea el input JSON recibido."""
    log("üì• Recibida nueva petici√≥n de inferencia")

    try:
        data = json.loads(request_body)
        log(f"üî∏ Raw request: {data}")

        if "series" not in data:
            raise ValueError("Falta la clave requerida: 'series'")

        series = data["series"]
        pred_len = data.get("prediction_length", 3)

        log(f"‚û°Ô∏è Series length: {len(series)} | Prediction length: {pred_len}")
        return series, pred_len

    except Exception as e:
        log(f"‚ùå Error parsing input: {e}")
        raise


def predict_fn(data, model):
    """Realiza la inferencia."""
    start = time.time()
    series, pred_len = data

    # Asegurar formato tensor
    if isinstance(series[0], (float, int)):
        series = [series]

    series_tensor = torch.tensor(series, dtype=torch.float32)

    log(f"‚öôÔ∏è Ejecutando predicci√≥n | input shape: {series_tensor.shape}")
    quantiles, out = model.predict_quantiles(series_tensor, prediction_length=pred_len)

    elapsed = time.time() - start
    log(f"‚úÖ Predicci√≥n completada en {elapsed:.2f}s")

    # Log de ejemplo
    sample = quantiles.tolist()[0]
    log(f"üîπ Ejemplo forecast (primera serie, 3 valores): {sample[:3]}")

    return quantiles.tolist(), out.tolist()


def output_fn(prediction, accept):
    """Formatea la salida como JSON."""
    response = {"forecast": prediction}
    log(f"üì§ Enviando respuesta: keys={list(response.keys())}, size={len(json.dumps(response))} bytes")
    return json.dumps(response)


# --------------------------
# MODO LOCAL / TEST
# --------------------------
if __name__ == "__main__":
    # Simular request local
    request_body = json.dumps({
        "series": [[10.0, 20.0, 30.0, 40.0, 50.0]],
        "prediction_length": 3
    })

    log("üß† Modo local detectado. Iniciando prueba...")

    model = model_fn(MODEL_DIR)
    inputs = input_fn(request_body, "application/json")
    quantiles, preds = predict_fn(inputs, model)
    output = output_fn(preds, "application/json")

    print("\n=== üìà OUTPUT ===")
    print(output)
    print("================\n")

    # Mantener contenedor vivo si est√°s debuggeando
    log("üåÄ Manteniendo contenedor activo (Ctrl+C para salir)...")
    while True:
        time.sleep(60)
